{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline For Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO RUN THIS NOTEBOOK, YOU NEED TO HAVE THE ipykernel PACKAGE INSTALLED\n",
    "# YOU CAN INSTALL IT BY RUNNING `pip install ipykernel`\n",
    "# OR BY UNCOMMENTING THE LINE BELOW AND RUNNING THE CELL\n",
    "# You also need to have the requirements.txt installed\n",
    "# !pip install ipykernel\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedTokenizerFast,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "\n",
    "\n",
    "# import torch.nn as nn\n",
    "\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"../data/train.csv\"\n",
    "VALIDATION_PATH = \"../data/validation.csv\"\n",
    "TEST_PATH = \"../data/test_text.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "validation_df = pd.read_csv(VALIDATION_PATH)\n",
    "\n",
    "# The Test data corresponds to the file for submission on the Kaggle Dataset for which the labels are not available\n",
    "test_df = pd.read_csv(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (21401, 2)\n",
      "                                                text  \\\n",
      "0  Thierry Mariani sur la liste du Rassemblement ...   \n",
      "1  C'est désormais officiel : Alain Juppé n'est p...   \n",
      "2  La mesure est décriée par les avocats et les m...   \n",
      "3  Dans une interview accordée au Figaro mercredi...   \n",
      "4  Le préjudice est estimé à 2 millions d'euros. ...   \n",
      "\n",
      "                                              titles  \n",
      "0  L'information n'a pas été confirmée par l'inté...  \n",
      "1  Le maire de Bordeaux ne fait plus partie des R...  \n",
      "2  En 2020, les tribunaux d'instance fusionnent a...  \n",
      "3  Les médecins jugés \"gros prescripteurs d'arrêt...  \n",
      "4  Il aura fallu mobiliser 90 gendarmes pour cett...  \n",
      "\n",
      "\n",
      "Validation data shape:  (1500, 2)\n",
      "                                                text  \\\n",
      "0  Sur les réseaux sociaux, les images sont impre...   \n",
      "1  La vidéo est devenue virale. Elle montre un po...   \n",
      "2  Depuis la présidentielle, il est parfois un pe...   \n",
      "3  Routes endommagées, trains toujours perturbés,...   \n",
      "4  Une enquête menée par le journal l'Obs.La nuit...   \n",
      "\n",
      "                                              titles  \n",
      "0  Le bateau de croisière, long de 275 m, a percu...  \n",
      "1  Le parquet de Paris a annoncé vendredi avoir o...  \n",
      "2  À Trappes (Yvelines), c'est désormais la star....  \n",
      "3  Un homme de 44 ans est porté disparu depuis sa...  \n",
      "4  Son nom n'avait jusque-là jamais été cité dans...  \n"
     ]
    }
   ],
   "source": [
    "print(\"Train data shape: \", train_df.shape)\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Validation data shape: \", validation_df.shape)\n",
    "print(validation_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Functions\n",
    "\n",
    "The future functions for other implementations should keep the same input/output format for ease of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline From the dataset\n",
    "\n",
    "\n",
    "# Function that generates summaries using LEAD-N\n",
    "def lead_summary(text: pd.Series) -> List[Tuple[int, str]]:\n",
    "    \"\"\"Generate summaries using the LEAD-N method\n",
    "\n",
    "    ## Input :\n",
    "    - text : pd.Series : The text data for which the summaries are to be generated (the text column from the dataset)\n",
    "\n",
    "    ## Output :\n",
    "    - summaries : List[Tuple[int, str]] : A list of Tuples containing the index of the text and the summary generated using the LEAD-N method\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    for idx, row in text.items():\n",
    "        sentences = row.split(\".\")\n",
    "        summaries.append((idx, sentences[0] + \".\"))\n",
    "    return summaries\n",
    "\n",
    "\n",
    "# Function that generates summaries using EXT-ORACLE\n",
    "def ext_oracle_summary(\n",
    "    text: pd.Series,\n",
    "    titles: pd.Series,\n",
    "    scorer: rouge_scorer.RougeScorer,\n",
    ") -> List[Tuple[int, str]]:\n",
    "    \"\"\"Generate summaries using the EXT-ORACLE method\n",
    "\n",
    "    ## Input :\n",
    "    - text : pd.Series : The text data for which the summaries are to be generated (the text column from the dataset)\n",
    "    - titles : pd.Series : The titles of the text data (the titles column from the dataset)\n",
    "    - scorer : rouge_scorer.RougeScorer : The Rouge Scorer object\n",
    "\n",
    "    ## Output :\n",
    "    - summaries : List[Tuple[int, str]] : A list of Tuples containing the index of the text and the summary generated using the EXT-ORACLE method\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    for idx, row in text.items():\n",
    "        sentences = row.split(\".\")\n",
    "        reference = titles.iloc[idx]  # type: ignore\n",
    "        rs = [scorer.score(sentence, reference)[\"rougeL\"][2] for sentence in sentences]\n",
    "        index, _ = max(enumerate(rs), key=itemgetter(1))\n",
    "        summaries.append((idx, sentences[index]))\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the functions on the validation data\n",
    "\n",
    "lead_summary_validation = lead_summary(validation_df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lead Summary:  Sur les réseaux sociaux, les images sont impressionnantes.\n",
      "Reference Summary:  Le bateau de croisière, long de 275 m, a percuté un quai lors de son arrivée dans le port de Venise, dimanche 2 juin. Quatre personnes ont été blessées.\n",
      "\n",
      "\n",
      "Lead Summary:  La vidéo est devenue virale.\n",
      "Reference Summary:  Le parquet de Paris a annoncé vendredi avoir ouvert une enquête après la diffusion d'une vidéo dans laquelle un policier semble tirer quasi à bout portant sur des manifestants avec un LBD lors d'échauffourées jeudi pendant le défilé contre la réforme des retraites.\n",
      "\n",
      "\n",
      "Lead Summary:  Depuis la présidentielle, il est parfois un peu gêné de cette notoriété qui ne se traduira pas forcément dans les urnes.\n",
      "Reference Summary:  À Trappes (Yvelines), c'est désormais la star. Benoît Hamon, nouvelle idole, et surtout des enfants.\n",
      "\n",
      "\n",
      "Lead Summary:  Routes endommagées, trains toujours perturbés, agriculteurs désemparés : la Côte d'Azur tente en ce début de semaine de se remettre des dégâts causés par les pluies exceptionnelles et les inondations qui ont fait au moins quatre morts au cours du week-end.\n",
      "Reference Summary:  Un homme de 44 ans est porté disparu depuis samedi soir. Il était parti pêcher sur des rochers où il avait l'habitude d'aller, dans une zone abrupte balayée par de hautes vagues rendant la roche glissante, alors que le département avait été placé en vigilance orange pluie, inondations et risque de crue.\n",
      "\n",
      "\n",
      "Lead Summary:  Une enquête menée par le journal l'Obs.\n",
      "Reference Summary:  Son nom n'avait jusque-là jamais été cité dans ladite \"affaire Benalla\". Conseiller de l'ombre et très proche d'Emmanuel Macron, Ismaël Emelien pourrait avoir eu un rôle prépondérant la nuit où les premières images de la place de la Contrescarpe ont été rendues publiques.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, summary in lead_summary_validation[:5]:\n",
    "    print(\"Lead Summary: \", summary)\n",
    "    print(\"Reference Summary: \", validation_df[\"titles\"].iloc[idx])  # type: ignore\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_oracle_summary_validation = ext_oracle_summary(\n",
    "    validation_df[\"text\"],\n",
    "    validation_df[\"titles\"],\n",
    "    rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXT-ORACLE Summary:   Dimanche matin à Venise, l'équipage du MSC Opéra a perdu le contrôle du paquebot, à son arrivée dans le port de la cité des Doges\n",
      "Reference Summary:  Le bateau de croisière, long de 275 m, a percuté un quai lors de son arrivée dans le port de Venise, dimanche 2 juin. Quatre personnes ont été blessées.\n",
      "\n",
      "\n",
      "EXT-ORACLE Summary:   Elle montre un policier semblant tirer quasiment à bout portant sur des manifestations avec un lanceur de balles de défense (LBD) lors d'échauffourées survenues jeudi, pendant le défilé contre la réforme des retraites\n",
      "Reference Summary:  Le parquet de Paris a annoncé vendredi avoir ouvert une enquête après la diffusion d'une vidéo dans laquelle un policier semble tirer quasi à bout portant sur des manifestants avec un LBD lors d'échauffourées jeudi pendant le défilé contre la réforme des retraites.\n",
      "\n",
      "\n",
      "EXT-ORACLE Summary:   Mais à Trappes, ce n'est pas Emmanuel Macron la cible principale, mais bien Benoît Hamon\n",
      "Reference Summary:  À Trappes (Yvelines), c'est désormais la star. Benoît Hamon, nouvelle idole, et surtout des enfants.\n",
      "\n",
      "\n",
      "EXT-ORACLE Summary:  Samedi, l'homme serait parti pêcher sur des rochers où il avait l'habitude d'aller dans une zone abrupte balayée par de hautes vagues rendant la roche glissante\n",
      "Reference Summary:  Un homme de 44 ans est porté disparu depuis samedi soir. Il était parti pêcher sur des rochers où il avait l'habitude d'aller, dans une zone abrupte balayée par de hautes vagues rendant la roche glissante, alors que le département avait été placé en vigilance orange pluie, inondations et risque de crue.\n",
      "\n",
      "\n",
      "EXT-ORACLE Summary:   Selon l'Obs, le \"conseiller spécial\" du chef de l'État serait \"le plus en capacité d'avoir pu réceptionner durant la nuit le CD\" contenant les images de l'affrontement entre Alexandre Benalla et un couple de manifestants\n",
      "Reference Summary:  Son nom n'avait jusque-là jamais été cité dans ladite \"affaire Benalla\". Conseiller de l'ombre et très proche d'Emmanuel Macron, Ismaël Emelien pourrait avoir eu un rôle prépondérant la nuit où les premières images de la place de la Contrescarpe ont été rendues publiques.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, summary in ext_oracle_summary_validation[:5]:\n",
    "    print(\"EXT-ORACLE Summary: \", summary)\n",
    "    print(\"Reference Summary: \", validation_df[\"titles\"].iloc[idx])  # type: ignore\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_rouge_score(\n",
    "    summaries: List[Tuple[int, str]],\n",
    "    titles: pd.Series,\n",
    "    scorer: rouge_scorer.RougeScorer,\n",
    "):\n",
    "    \"\"\"Calculate the average rouge score for the summaries generated\n",
    "\n",
    "    ## Input :\n",
    "    - summaries : [(int, str)...] : A list of Tuples containing the index of the text and the summary generated\n",
    "    - text : pd.Series : The text data for which the summaries are to be generated (the text column from the dataset)\n",
    "    - scorer : rouge_scorer.RougeScorer : The Rouge Scorer object\n",
    "\n",
    "    ## Output :\n",
    "    - average_rouge : float : The average rouge score for the summaries generated\n",
    "    \"\"\"\n",
    "    rouge_scores = []\n",
    "    for idx, summary in summaries:\n",
    "        reference = titles.iloc[idx]  # type: ignore\n",
    "        rouge_scores.append(scorer.score(summary, reference)[\"rougeL\"][2])\n",
    "    return sum(rouge_scores) / len(rouge_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_list = [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]\n",
    "model_checkpoint = model_checkpoint_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shoud load the T5TokenizerFast from the transformers library\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "# t5_tokenizer = T5TokenizerFast.from_pretrained(model_checkpoint)\n",
    "\n",
    "# otherwise we can use thisone and compare the results\n",
    "# t5_tokenizer = T5Tokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5TokenizerFast(name_or_path='t5-small', vocab_size=32100, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32000: AddedToken(\"<extra_id_99>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32001: AddedToken(\"<extra_id_98>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32002: AddedToken(\"<extra_id_97>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32003: AddedToken(\"<extra_id_96>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32004: AddedToken(\"<extra_id_95>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32005: AddedToken(\"<extra_id_94>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32006: AddedToken(\"<extra_id_93>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32007: AddedToken(\"<extra_id_92>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32008: AddedToken(\"<extra_id_91>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32009: AddedToken(\"<extra_id_90>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32010: AddedToken(\"<extra_id_89>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32011: AddedToken(\"<extra_id_88>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32012: AddedToken(\"<extra_id_87>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32013: AddedToken(\"<extra_id_86>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32014: AddedToken(\"<extra_id_85>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32015: AddedToken(\"<extra_id_84>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32016: AddedToken(\"<extra_id_83>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32017: AddedToken(\"<extra_id_82>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32018: AddedToken(\"<extra_id_81>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32019: AddedToken(\"<extra_id_80>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32020: AddedToken(\"<extra_id_79>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32021: AddedToken(\"<extra_id_78>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32022: AddedToken(\"<extra_id_77>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32023: AddedToken(\"<extra_id_76>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32024: AddedToken(\"<extra_id_75>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32025: AddedToken(\"<extra_id_74>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32026: AddedToken(\"<extra_id_73>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32027: AddedToken(\"<extra_id_72>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32028: AddedToken(\"<extra_id_71>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32029: AddedToken(\"<extra_id_70>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32030: AddedToken(\"<extra_id_69>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32031: AddedToken(\"<extra_id_68>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32032: AddedToken(\"<extra_id_67>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32033: AddedToken(\"<extra_id_66>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32034: AddedToken(\"<extra_id_65>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32035: AddedToken(\"<extra_id_64>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32036: AddedToken(\"<extra_id_63>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32037: AddedToken(\"<extra_id_62>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32038: AddedToken(\"<extra_id_61>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32039: AddedToken(\"<extra_id_60>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32040: AddedToken(\"<extra_id_59>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32041: AddedToken(\"<extra_id_58>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32042: AddedToken(\"<extra_id_57>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32043: AddedToken(\"<extra_id_56>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32044: AddedToken(\"<extra_id_55>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32045: AddedToken(\"<extra_id_54>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32046: AddedToken(\"<extra_id_53>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32047: AddedToken(\"<extra_id_52>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32048: AddedToken(\"<extra_id_51>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32049: AddedToken(\"<extra_id_50>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32050: AddedToken(\"<extra_id_49>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32051: AddedToken(\"<extra_id_48>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32052: AddedToken(\"<extra_id_47>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32053: AddedToken(\"<extra_id_46>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32054: AddedToken(\"<extra_id_45>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32055: AddedToken(\"<extra_id_44>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32056: AddedToken(\"<extra_id_43>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32057: AddedToken(\"<extra_id_42>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32058: AddedToken(\"<extra_id_41>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32059: AddedToken(\"<extra_id_40>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32060: AddedToken(\"<extra_id_39>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32061: AddedToken(\"<extra_id_38>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32062: AddedToken(\"<extra_id_37>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32063: AddedToken(\"<extra_id_36>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32064: AddedToken(\"<extra_id_35>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32065: AddedToken(\"<extra_id_34>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32066: AddedToken(\"<extra_id_33>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32067: AddedToken(\"<extra_id_32>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32068: AddedToken(\"<extra_id_31>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32069: AddedToken(\"<extra_id_30>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32070: AddedToken(\"<extra_id_29>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32071: AddedToken(\"<extra_id_28>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32072: AddedToken(\"<extra_id_27>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32073: AddedToken(\"<extra_id_26>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32074: AddedToken(\"<extra_id_25>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32075: AddedToken(\"<extra_id_24>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32076: AddedToken(\"<extra_id_23>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32077: AddedToken(\"<extra_id_22>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32078: AddedToken(\"<extra_id_21>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32079: AddedToken(\"<extra_id_20>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32080: AddedToken(\"<extra_id_19>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32081: AddedToken(\"<extra_id_18>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32082: AddedToken(\"<extra_id_17>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32083: AddedToken(\"<extra_id_16>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32084: AddedToken(\"<extra_id_15>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32085: AddedToken(\"<extra_id_14>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32086: AddedToken(\"<extra_id_13>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32087: AddedToken(\"<extra_id_12>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32088: AddedToken(\"<extra_id_11>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32089: AddedToken(\"<extra_id_10>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32090: AddedToken(\"<extra_id_9>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32091: AddedToken(\"<extra_id_8>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32092: AddedToken(\"<extra_id_7>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32093: AddedToken(\"<extra_id_6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32094: AddedToken(\"<extra_id_5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32095: AddedToken(\"<extra_id_4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32096: AddedToken(\"<extra_id_3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32097: AddedToken(\"<extra_id_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32098: AddedToken(\"<extra_id_1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32099: AddedToken(\"<extra_id_0>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "<class 'transformers.models.t5.tokenization_t5_fast.T5TokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "print(t5_tokenizer)\n",
    "print(type(t5_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should load the T5ForConditionalGeneration model from the transformers library\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32128, 512)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 8)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-5): 5 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 8)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-5): 5 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
      ")\n",
      "<class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"../outputs//{model_name}-finetuned\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=1,\n",
    "    eval_steps=2,\n",
    "    save_steps=2,\n",
    "    warmup_steps=1,\n",
    "    # overwrite_output_dir=True,\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(\n",
    "    text: str,\n",
    "    title: str,\n",
    "    t5_tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast,\n",
    "    prefix: str = \"summarize: \",\n",
    "    max_input_length: int = 1024,\n",
    "    max_target_length: int = 64,\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Preprocess the text data\n",
    "\n",
    "    ## Input :\n",
    "    - text : str : The text data to be preprocessed\n",
    "    - title : str : The title of the text data, The Target\n",
    "    - prefix : str : The prefix to be added to the text data as T5 model can be used for translation as well\n",
    "    - max_input_length : int : The maximum length of the input text\n",
    "    - max_target_length : int : The maximum length of the target text\n",
    "    - tokenizer : t5_tokenizer : The tokenizer object\n",
    "\n",
    "    ## Output :\n",
    "    - model_inputs : Dict[str, Union[torch.Tensor, None]] : The model inputs\n",
    "    \"\"\"\n",
    "    inputs = t5_tokenizer(\n",
    "        f\"{prefix} {text}\",\n",
    "        max_length=max_input_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    targets = t5_tokenizer(\n",
    "        title,\n",
    "        max_length=max_target_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    model_inputs = {\n",
    "        \"input_ids\": inputs.input_ids,\n",
    "        \"attention_mask\": inputs.attention_mask,\n",
    "        \"labels\": targets.input_ids,\n",
    "    }\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_from_df(df: pd.DataFrame):\n",
    "    dataframe_list = []\n",
    "    for i in range(len(df)):\n",
    "        dataframe_list.append(\n",
    "            preprocess_text(df[\"text\"].iloc[i], df[\"titles\"].iloc[i], t5_tokenizer)\n",
    "        )\n",
    "\n",
    "    return Dataset.from_list(dataframe_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct metric\n",
    "rouge = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = t5_tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = t5_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    rouge_scores = [\n",
    "        rouge.score(pred, label)[\"rougeL\"].fmeasure\n",
    "        for pred, label in zip(decoded_preds, decoded_labels)\n",
    "    ]\n",
    "\n",
    "    return {\"rougeL_fmeasure\": sum(rouge_scores) / len(rouge_scores)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the DataCollector\n",
    "data_collator = DataCollatorForSeq2Seq(t5_tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = preprocess_from_df(train_df)\n",
    "validation_dataset = preprocess_from_df(validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset = DatasetDict({\"train\": train_dataset, \"validation\": validation_dataset})\n",
    "total_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shaamallow/.miniforge3/envs/datachallenge/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Construct the Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=total_dataset[\"train\"],\n",
    "    eval_dataset=total_dataset[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 500/2676 [06:59<24:13,  1.50it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.159, 'grad_norm': 1.5339239835739136, 'learning_rate': 4.067289719626168e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 1000/2676 [13:28<18:08,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.973, 'grad_norm': 1.4470257759094238, 'learning_rate': 3.1327102803738314e-05, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 1500/2676 [20:01<12:57,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9215, 'grad_norm': 1.987421989440918, 'learning_rate': 2.1981308411214954e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 2000/2676 [26:32<08:52,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.917, 'grad_norm': 2.0878636837005615, 'learning_rate': 1.2635514018691589e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 2500/2676 [35:48<05:24,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9142, 'grad_norm': 1.627769947052002, 'learning_rate': 3.2897196261682244e-06, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 2675/2676 [42:07<00:03,  3.67s/it]/home/shaamallow/.miniforge3/envs/datachallenge/lib/python3.11/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      "100%|██████████| 2676/2676 [42:53<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7585475444793701, 'eval_rougeL_fmeasure': 0.166479094478431, 'eval_runtime': 44.9549, 'eval_samples_per_second': 33.367, 'eval_steps_per_second': 4.182, 'epoch': 1.0}\n",
      "{'train_runtime': 2573.4343, 'train_samples_per_second': 8.316, 'train_steps_per_second': 1.04, 'train_loss': 1.9709011129199656, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2676, training_loss=1.9709011129199656, metrics={'train_runtime': 2573.4343, 'train_samples_per_second': 8.316, 'train_steps_per_second': 1.04, 'train_loss': 1.9709011129199656, 'epoch': 1.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t5_summary(\n",
    "    text: pd.Series, t5_tokenizer: PreTrainedTokenizer, model: AutoModelForSeq2SeqLM\n",
    "):\n",
    "    \"\"\"Generate summaries using the T5 model\n",
    "\n",
    "    Using the standard representation defined in baseline part of notebook\n",
    "\n",
    "    ## Input :\n",
    "    - text : pd.Series : The text data for which the summaries are to be generated (the text column from the dataset)\n",
    "    - t5_tokenizer : PreTrainedTokenizer : The tokenizer object\n",
    "    - model : AutoModelForSeq2SeqLM : The model object\n",
    "\n",
    "    ## Output :\n",
    "    - summaries : List[Tuple[int, str]] : A list of Tuples containing the index of the text and the summary generated using the T5 model\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    for idx, row in tqdm(text.items()):\n",
    "        input_text = t5_tokenizer(row, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "        output = model.generate(\n",
    "            input_text,\n",
    "            max_length=64,\n",
    "            early_stopping=True,\n",
    "            num_return_sequences=1,\n",
    "        )\n",
    "        summaries.append(\n",
    "            (idx, t5_tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "        )\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/shaamallow/.miniforge3/envs/datachallenge/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "1500it [05:42,  4.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run title generation for submission\n",
    "t5_summary_kaggle = t5_summary(test_df[\"text\"], t5_tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_summary_kaggle_df = pd.DataFrame(t5_summary_kaggle, columns=[\"ID\", \"titles\"])\n",
    "t5_summary_kaggle_df.to_csv(\"../outputs/submissions//t5_summary_kaggle.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [21603, 10, 100, 19, 3, 9, 794, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [100, 19, 3, 9, 794, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_text(\"This is a test\", \"This is a test\", t5_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thierry Mariani sur la liste du Rassemblement national (RN, ex-FN) aux européennes ? C'est ce qu'affirme mardi 11 septembre Chez Pol, la nouvelle newsletter politique de Libération. L'ancien député Les Républicain et ministre de Nicolas Sarkozy serait sur le point de rejoindre les troupes de Marine Le Pen pour le élections européennes de 2019. \"Ça va se faire. Ce n'est plus qu'une question de calendrier. On n'est pas obligé de l'annoncer tout de suite, à huit mois des européennes\", aurait ainsi assuré un membre influent du RN. Contacté par Franceinfo, M. Mariani n'a pas confirmé l'information. \"Les élections sont en juin, je ne sais même pas qui sera numéro 1 sur la liste\", a répondu l'ancien ministre des Transports. Il reconnaît toutefois, toujours cité par Franceinfo, que son nom sur la liste du RN \"fait partie des possibilités\". \"Fréjus est une ville sympathique mais je n'ai pas prévu de m'y rendre ce week_end\", a-t-il par ailleurs commenté sur Twitter alors que Marine Le Pen réunit les cadres de son parti ce week-end dans la cité varoise. Une proximité connue avec le FNLa proximité de Thierry Mariani avec le parti frontiste n'est pas nouvelle. \"Sans alliés, nous allons rester dans l'opposition pour longtemps. Il est temps de renverser la table. Le Front national a évolué. Regardons si un accord ou un rapprochement sont possibles\", avait-il déclaré dans une interview donnée au Journal du Dimanche en mars dernier. Puis, en avril, avait bruissé la rumeur d'un rencontre entre l'ex-député et Marine Le Pen, qui lui aurait proposé de figurer en position éligible sur la liste de son parti aux européennes. \"Pas de conclusion hâtive\", avait-il à l'époque écrit sur Twitter. Le même mois, Thierry Mariani avait cosigné une tribune publiée dans Valeurs actuelles aux côté d'élus frontistes appelant à une union des droites.\n",
      "L'information n'a pas été confirmée par l'intéressé qui déclare toutefois étudier la question.\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "summarize: Thierry Mariani sur la liste du Rassemblement national (RN, ex-FN) aux européennes? C'est ce qu'affirme mardi 11 septembre Chez Pol, la nouvelle newsletter politique de Libération. L'ancien député Les Républicain et ministre de Nicolas Sarkozy serait sur le point de rejoindre les troupes de Marine Le Pen pour le élections européennes de 2019. \"<unk>a va se faire. Ce n'est plus qu'une question de calendrier. On n'est pas obligé de l'annoncer tout de suite, à huit mois des européennes\", aurait ainsi assuré un membre influent du RN. Contacté par Franceinfo, M. Mariani n'a pas confirmé l'information. \"Les élections sont en juin, je ne sais même pas qui sera numéro 1 sur la liste\", a répondu l'ancien ministre des Transports. Il reconnaît toutefois, toujours cité par Franceinfo, que son nom sur la liste du RN \"fait partie des possibilités\". \"Fréjus est une ville sympathique mais je n'ai pas prévu de m'y rendre ce week_end\", a-t-il par ailleurs commenté sur Twitter alors que Marine Le Pen réunit les cadres de son parti ce week-end dans la cité varoise. Une proximité connue avec le FNLa proximité de Thierry Mariani avec le parti frontiste n'est pas nouvelle. \"Sans alliés, nous allons rester dans l'opposition pour longtemps. Il est temps de renverser la table. Le Front national a évolué. Regardons si un accord ou un rapprochement sont possibles\", avait-il déclaré dans une interview donnée au Journal du Dimanche en mars dernier. Puis, en avril, avait bruissé la rumeur d'un rencontre entre l'ex-député et Marine Le Pen, qui lui aurait proposé de figurer en position éligible sur la liste de son parti aux européennes. \"Pas de conclusion hâtive\", avait-il à l'époque écrit sur Twitter. Le même mois, Thierry Mariani avait cosigné une tribune publiée dans Valeurs actuelles aux côté d'élus frontistes appelant à une union des droites.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "L'information n'a pas été confirmée par l'intéressé qui déclare toutefois étudier la question.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "# Check Encoding/Decoding size embeddings\n",
    "test_text = train_df[\"text\"].iloc[0]\n",
    "test_title = train_df[\"titles\"].iloc[0]\n",
    "\n",
    "print(test_text)\n",
    "print(test_title)\n",
    "\n",
    "preprocess_test = preprocess_text(test_text, test_title, t5_tokenizer)\n",
    "\n",
    "print(preprocess_test.keys())\n",
    "\n",
    "decoded_text = t5_tokenizer.decode(preprocess_test[\"input_ids\"])\n",
    "decoded_title = t5_tokenizer.decode(preprocess_test[\"labels\"])\n",
    "\n",
    "print(decoded_text)\n",
    "print(decoded_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,   312,   187,    60,    20, 20820,     3,     9,  1136,    29,\n",
      "           106,    75,   154,    73, 26263,  1194,   285,     3,     7,    31,\n",
      "            32,   102,  2970,     6,     3,  5928,   759,     6,    20,    50,\n",
      "         16872,  2676,     3,    40,    31, 10398, 14523, 16872,     5,     1]],\n",
      "       device='cuda:0')\n",
      "Le maire de Bordeaux a dénoncé un glissement qui s'opère, selon lui, de la droite vers l'extême droite.\n",
      "Le maire de Bordeaux ne fait plus partie des Républicains et il tient à le montrer. Lors de ses voeux à la presse, l'édile a pris ses distances avec sa famille politique historique, qu'il trouve trop proche de Marine Le Pen.\n",
      "C'est désormais officiel : Alain Juppé n'est plus membre des Républicains. L'ex-Premier ministre de Jacques Chirac, cofondateur de l'UMP en 2002, ne paie plus sa cotisation auprès du parti de droite. Mercredi 9 janvier, le maire de Bordeaux a dénoncé un glissement qui s'opère, selon lui, de la droite vers l'extême droite. \"Je me reconnais de moins en moins dans cette famille politique, à laquelle je suis pourtant très attaché (...). C'est avec tristesse que je l'ai quittée, mais il y a une dérive vers des thèses qui sont celles très proches de l'extrême droite, et une ambiguïté sur l'Europe\", a-t-il déclaré face aux journalistes, réunis pour assister à ses voeux. \"On assiste à cette espèce de transfusion régulière, et sur les thèmes de fond, il y a des moments où je me demande qui j'entends à la radio ? Un membre de LR ou du RN ?\", a insisté le maire de Bordeaux. Le même jour, l'ex-député Thierry Mariani annonçait son départ de LR pour rallier une liste du Rassemblement national aux européennes de mai prochain. \"Cela fait deux ans que j'ai dit que je prenais mes distances avec Les Républicains. Les choses sont acquises depuis bien longtemps\", a tranché Alain Juppé. L'ancien candidat à la primaire de la droite pour la présidentielle de 2017 n'a jamais fait mystère de son désaccord avec les positions du président des Républicains, Laurent Wauquiez.\n"
     ]
    }
   ],
   "source": [
    "test_text = train_df[\"text\"].iloc[1]\n",
    "test_title = train_df[\"titles\"].iloc[1]\n",
    "\n",
    "input_text = t5_tokenizer(test_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "output = model.generate(\n",
    "    input_text,\n",
    "    max_length=64,\n",
    "    early_stopping=True,\n",
    "    num_return_sequences=1,\n",
    ")\n",
    "print(output)\n",
    "print(t5_tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "print(test_title)\n",
    "\n",
    "print(test_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datachallenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
