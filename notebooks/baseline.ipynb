{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline For Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO RUN THIS NOTEBOOK, YOU NEED TO HAVE THE ipykernel PACKAGE INSTALLED\n",
    "# YOU CAN INSTALL IT BY RUNNING `pip install ipykernel`\n",
    "# OR BY UNCOMMENTING THE LINE BELOW AND RUNNING THE CELL\n",
    "# You also need to have the requirements.txt installed\n",
    "# !pip install ipykernel\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedTokenizerFast,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "\n",
    "\n",
    "# import torch.nn as nn\n",
    "\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"../data/train.csv\"\n",
    "VALIDATION_PATH = \"../data/validation.csv\"\n",
    "TEST_PATH = \"../data/test_text.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "validation_df = pd.read_csv(VALIDATION_PATH)\n",
    "\n",
    "# The Test data corresponds to the file for submission on the Kaggle Dataset for which the labels are not available\n",
    "test_df = pd.read_csv(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train data shape: \", train_df.shape)\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Validation data shape: \", validation_df.shape)\n",
    "print(validation_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Functions\n",
    "\n",
    "The future functions for other implementations should keep the same input/output format for ease of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline From the dataset\n",
    "\n",
    "\n",
    "# Function that generates summaries using LEAD-N\n",
    "def lead_summary(text: pd.Series) -> List[Tuple[int, str]]:\n",
    "    \"\"\"Generate summaries using the LEAD-N method\n",
    "\n",
    "    ## Input :\n",
    "    - text : pd.Series : The text data for which the summaries are to be generated (the text column from the dataset)\n",
    "\n",
    "    ## Output :\n",
    "    - summaries : List[Tuple[int, str]] : A list of Tuples containing the index of the text and the summary generated using the LEAD-N method\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    for idx, row in text.items():\n",
    "        sentences = row.split(\".\")\n",
    "        summaries.append((idx, sentences[0] + \".\"))\n",
    "    return summaries\n",
    "\n",
    "\n",
    "# Function that generates summaries using EXT-ORACLE\n",
    "def ext_oracle_summary(\n",
    "    text: pd.Series,\n",
    "    titles: pd.Series,\n",
    "    scorer: rouge_scorer.RougeScorer,\n",
    ") -> List[Tuple[int, str]]:\n",
    "    \"\"\"Generate summaries using the EXT-ORACLE method\n",
    "\n",
    "    ## Input :\n",
    "    - text : pd.Series : The text data for which the summaries are to be generated (the text column from the dataset)\n",
    "    - titles : pd.Series : The titles of the text data (the titles column from the dataset)\n",
    "    - scorer : rouge_scorer.RougeScorer : The Rouge Scorer object\n",
    "\n",
    "    ## Output :\n",
    "    - summaries : List[Tuple[int, str]] : A list of Tuples containing the index of the text and the summary generated using the EXT-ORACLE method\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    for idx, row in text.items():\n",
    "        sentences = row.split(\".\")\n",
    "        reference = titles.iloc[idx]  # type: ignore\n",
    "        rs = [scorer.score(sentence, reference)[\"rougeL\"][2] for sentence in sentences]\n",
    "        index, _ = max(enumerate(rs), key=itemgetter(1))\n",
    "        summaries.append((idx, sentences[index]))\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the functions on the validation data\n",
    "\n",
    "lead_summary_validation = lead_summary(validation_df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, summary in lead_summary_validation[:5]:\n",
    "    print(\"Lead Summary: \", summary)\n",
    "    print(\"Reference Summary: \", validation_df[\"titles\"].iloc[idx])  # type: ignore\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_oracle_summary_validation = ext_oracle_summary(\n",
    "    validation_df[\"text\"],\n",
    "    validation_df[\"titles\"],\n",
    "    rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, summary in ext_oracle_summary_validation[:5]:\n",
    "    print(\"EXT-ORACLE Summary: \", summary)\n",
    "    print(\"Reference Summary: \", validation_df[\"titles\"].iloc[idx])  # type: ignore\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_rouge_score(\n",
    "    summaries: List[Tuple[int, str]],\n",
    "    titles: pd.Series,\n",
    "    scorer: rouge_scorer.RougeScorer,\n",
    "):\n",
    "    \"\"\"Calculate the average rouge score for the summaries generated\n",
    "\n",
    "    ## Input :\n",
    "    - summaries : [(int, str)...] : A list of Tuples containing the index of the text and the summary generated\n",
    "    - text : pd.Series : The text data for which the summaries are to be generated (the text column from the dataset)\n",
    "    - scorer : rouge_scorer.RougeScorer : The Rouge Scorer object\n",
    "\n",
    "    ## Output :\n",
    "    - average_rouge : float : The average rouge score for the summaries generated\n",
    "    \"\"\"\n",
    "    rouge_scores = []\n",
    "    for idx, summary in summaries:\n",
    "        reference = titles.iloc[idx]  # type: ignore\n",
    "        rouge_scores.append(scorer.score(summary, reference)[\"rougeL\"][2])\n",
    "    return sum(rouge_scores) / len(rouge_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_list = [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]\n",
    "model_checkpoint = model_checkpoint_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shoud load the T5TokenizerFast from the transformers library\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "# t5_tokenizer = T5TokenizerFast.from_pretrained(model_checkpoint)\n",
    "\n",
    "# otherwise we can use thisone and compare the results\n",
    "# t5_tokenizer = T5Tokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t5_tokenizer)\n",
    "print(type(t5_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should load the T5ForConditionalGeneration model from the transformers library\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"../outputs//{model_name}-finetuned\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=1,\n",
    "    eval_steps=2,\n",
    "    save_steps=2,\n",
    "    warmup_steps=1,\n",
    "    # overwrite_output_dir=True,\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(\n",
    "    text: str,\n",
    "    title: str,\n",
    "    t5_tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast,\n",
    "    prefix: str = \"summarize: \",\n",
    "    max_input_length: int = 1024,\n",
    "    max_target_length: int = 64,\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Preprocess the text data\n",
    "\n",
    "    ## Input :\n",
    "    - text : str : The text data to be preprocessed\n",
    "    - title : str : The title of the text data, The Target\n",
    "    - prefix : str : The prefix to be added to the text data as T5 model can be used for translation as well\n",
    "    - max_input_length : int : The maximum length of the input text\n",
    "    - max_target_length : int : The maximum length of the target text\n",
    "    - tokenizer : t5_tokenizer : The tokenizer object\n",
    "\n",
    "    ## Output :\n",
    "    - model_inputs : Dict[str, Union[torch.Tensor, None]] : The model inputs\n",
    "    \"\"\"\n",
    "    inputs = t5_tokenizer(\n",
    "        f\"{prefix} {text}\",\n",
    "        max_length=max_input_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    targets = t5_tokenizer(\n",
    "        title,\n",
    "        max_length=max_target_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    model_inputs = {\n",
    "        \"input_ids\": inputs.input_ids,\n",
    "        \"attention_mask\": inputs.attention_mask,\n",
    "        \"labels\": targets.input_ids,\n",
    "    }\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_from_df(df: pd.DataFrame):\n",
    "    dataframe_list = []\n",
    "    for i in range(len(df)):\n",
    "        dataframe_list.append(\n",
    "            preprocess_text(df[\"text\"].iloc[i], df[\"titles\"].iloc[i], t5_tokenizer)\n",
    "        )\n",
    "\n",
    "    return Dataset.from_list(dataframe_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct metric\n",
    "rouge = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = t5_tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = t5_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    rouge_scores = [\n",
    "        rouge.score(pred, label)[\"rougeL\"].fmeasure\n",
    "        for pred, label in zip(decoded_preds, decoded_labels)\n",
    "    ]\n",
    "\n",
    "    return {\"rougeL_fmeasure\": sum(rouge_scores) / len(rouge_scores)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the DataCollector\n",
    "data_collator = DataCollatorForSeq2Seq(t5_tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = preprocess_from_df(train_df)\n",
    "validation_dataset = preprocess_from_df(validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset = DatasetDict({\"train\": train_dataset, \"validation\": validation_dataset})\n",
    "total_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=total_dataset[\"train\"],\n",
    "    eval_dataset=total_dataset[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t5_summary(\n",
    "    text: pd.Series, t5_tokenizer: PreTrainedTokenizer, model: AutoModelForSeq2SeqLM\n",
    "):\n",
    "    \"\"\"Generate summaries using the T5 model\n",
    "\n",
    "    Using the standard representation defined in baseline part of notebook\n",
    "\n",
    "    ## Input :\n",
    "    - text : pd.Series : The text data for which the summaries are to be generated (the text column from the dataset)\n",
    "    - t5_tokenizer : PreTrainedTokenizer : The tokenizer object\n",
    "    - model : AutoModelForSeq2SeqLM : The model object\n",
    "\n",
    "    ## Output :\n",
    "    - summaries : List[Tuple[int, str]] : A list of Tuples containing the index of the text and the summary generated using the T5 model\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    for idx, row in tqdm(text.items()):\n",
    "        input_text = t5_tokenizer(row, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "        output = model.generate(\n",
    "            input_text,\n",
    "            max_length=64,\n",
    "            early_stopping=True,\n",
    "            num_return_sequences=1,\n",
    "        )\n",
    "        summaries.append(\n",
    "            (idx, t5_tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "        )\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from pretrained\n",
    "\n",
    "# model_name = model_checkpoint.split(\"/\")[-1]\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    f\"../outputs/{model_name}-finetuned/checkpoint-2676\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run title generation for submission\n",
    "t5_summary_kaggle = t5_summary(test_df[\"text\"], t5_tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_summary_kaggle_df = pd.DataFrame(t5_summary_kaggle, columns=[\"ID\", \"titles\"])\n",
    "t5_summary_kaggle_df.to_csv(\"../outputs/submissions//t5_summary_kaggle.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocess_text(\"This is a test\", \"This is a test\", t5_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Encoding/Decoding size embeddings\n",
    "test_text = train_df[\"text\"].iloc[0]\n",
    "test_title = train_df[\"titles\"].iloc[0]\n",
    "\n",
    "print(test_text)\n",
    "print(test_title)\n",
    "\n",
    "preprocess_test = preprocess_text(test_text, test_title, t5_tokenizer)\n",
    "\n",
    "print(preprocess_test.keys())\n",
    "\n",
    "decoded_text = t5_tokenizer.decode(preprocess_test[\"input_ids\"])\n",
    "decoded_title = t5_tokenizer.decode(preprocess_test[\"labels\"])\n",
    "\n",
    "print(decoded_text)\n",
    "print(decoded_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = train_df[\"text\"].iloc[1]\n",
    "test_title = train_df[\"titles\"].iloc[1]\n",
    "\n",
    "input_text = t5_tokenizer(test_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "output = model.generate(\n",
    "    input_text,\n",
    "    max_length=64,\n",
    "    early_stopping=True,\n",
    "    num_return_sequences=1,\n",
    ")\n",
    "print(output)\n",
    "print(t5_tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "print(test_title)\n",
    "\n",
    "print(test_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datachallenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
