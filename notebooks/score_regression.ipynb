{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rouge-L score prediction via regression\n",
    "\n",
    "The goal is to use spacy in order to identify word tags in sentences, and use the resulting parsing in order to find the sentence that best summarizes the text it is from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from src.metrics import single_rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.load_data import load_data\n",
    "\n",
    "train_df, validation_df, test_df = load_data()\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count tokens appearing in relevant & irrelevant sentences in order to balance the scores of each tag\n",
    "relevant_tag_count = 0\n",
    "irrelevant_tag_count = 0\n",
    "\n",
    "relevant_tag_counts: dict[str, int] = {}\n",
    "irrelevant_tag_counts: dict[str, int] = {}\n",
    "\n",
    "\n",
    "def text_to_sentences(text: str) -> list[str]:\n",
    "    return [s.strip() for s in text.split(\".\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy POS tagging\n",
    "\n",
    "Using SpaCy to parse the text and identify the parts of speech in the text. The parts of speech are then used to identify the most important words in the text.\n",
    "\n",
    "ISSUE: too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download fr_dep_news_trf\n",
    "\n",
    "import spacy\n",
    "\n",
    "tagger = spacy.load(\"fr_dep_news_trf\")\n",
    "\n",
    "\n",
    "def extract_tags(text: str, counter: dict[str, int]) -> int:\n",
    "    \"\"\"Add the found tags to the argument counter dictionary.\"\"\"\n",
    "\n",
    "    tags = tagger(text)\n",
    "    for tag in tags:\n",
    "        name = tag.pos_\n",
    "\n",
    "        if name in counter:\n",
    "            counter[name] += 1\n",
    "        else:\n",
    "            counter[name] = 1\n",
    "    return len(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every text - target pair, we identify the sentence with the best Rouge-L score relative to the target, and we count for each token how much it appears in best sentences vs the other ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = train_df.shape[0]\n",
    "\n",
    "for _, (text, target) in tqdm(train_df.iterrows(), total=nrows):\n",
    "\n",
    "    sentences = text_to_sentences(text)\n",
    "    rouge_scores = [single_rouge_score(target, sentence)\n",
    "                    for sentence in sentences]\n",
    "\n",
    "    # Extract the index of the best sentence score\n",
    "    best_sentence_index = rouge_scores.index(max(rouge_scores))\n",
    "\n",
    "    # Count all tokens for all sentences. etc.\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if i == best_sentence_index:\n",
    "            relevant_tag_count += extract_tags(sentence, relevant_tag_counts)\n",
    "        else:\n",
    "            irrelevant_tag_count += extract_tags(sentence,\n",
    "                                                 irrelevant_tag_counts)\n",
    "\n",
    "# Not viable: too long !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence & Paragraph Embeddings\n",
    "\n",
    "We embed paragraphs and sentences using pretrained models. We then use a regressor from `scikit-learn` to predict the Rouge-L score of each sentence, and thus pick the best summarizing one using the max Rouge-L score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_compute(name: str, df: pd.DataFrame):\n",
    "\n",
    "    embeddings_filename = f\"data/{name}_embeddings.npy\"\n",
    "    scores_filename = f\"data/{name}_scores.npy\"\n",
    "\n",
    "    if os.path.exists(embeddings_filename) and os.path.exists(scores_filename):\n",
    "        return np.load(embeddings_filename), np.load(scores_filename)\n",
    "\n",
    "    # Else: do and save\n",
    "    scores: list[float] = []\n",
    "    final_embeddings = []\n",
    "\n",
    "    nrows = train_df.shape[0]\n",
    "\n",
    "    for _, (text, target) in tqdm(train_df.iterrows(), total=nrows):\n",
    "\n",
    "        # Extract sentences\n",
    "        sentences = text_to_sentences(text)\n",
    "\n",
    "        # Compute Rouge-L scores relative to the target\n",
    "        rouge_scores = [single_rouge_score(\n",
    "            target, sentence) for sentence in sentences]\n",
    "        scores.extend(rouge_scores)\n",
    "\n",
    "        # Compute embeddings\n",
    "        sentence_embeddings = np.array(model.encode(sentences))\n",
    "        paragraph_embeddings = np.array(model.encode(text))\n",
    "\n",
    "        # Stack sentence embeddings with their respective paragraph embedding,\n",
    "        # into the global sentence_embeddings list\n",
    "        repeated_paragraph = np.tile(paragraph_embeddings, (len(sentences), 1))\n",
    "        embeddings = np.concatenate(\n",
    "            (sentence_embeddings, repeated_paragraph), axis=1)\n",
    "\n",
    "        final_embeddings.extend(embeddings)\n",
    "\n",
    "    np.save(embeddings_filename, final_embeddings)\n",
    "    np.save(scores_filename, scores)\n",
    "    return np.array(final_embeddings), np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embed, train_scores = load_or_compute(\"train\", train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_embed, valid_scores = load_or_compute(\"valid\", validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize shapes\n",
    "print(train_embed.shape, train_scores.shape)\n",
    "print(valid_embed.shape, valid_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different regression models\n",
    "ridge_reg = Ridge(alpha=1.0)\n",
    "lasso_reg = Lasso(alpha=1.0)\n",
    "elastic_reg = ElasticNet(alpha=1.0, l1_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg.fit(train_embed, train_scores)\n",
    "print(ridge_reg.score(valid_embed, valid_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reg.fit(train_embed, train_scores)\n",
    "print(lasso_reg.score(valid_embed, valid_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_reg.fit(train_embed, train_scores)\n",
    "print(elastic_reg.score(valid_embed, valid_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then test one of the regressors on the validation data in order to pick a sentence for each paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_sentences(embeddings: np.ndarray, df: pd.DataFrame):\n",
    "    \"\"\"Given a dataset as DataFrame and precomputed embeddings, pick and test a sentence for each\n",
    "    text to be summarized.\"\"\"\n",
    "\n",
    "    nrows = df.shape[0]\n",
    "    best_sentences: list[str] = []\n",
    "\n",
    "    # Embedding span pointers\n",
    "    start = 0\n",
    "\n",
    "    for _, (text, *_) in tqdm(df.iterrows(), total=nrows):\n",
    "\n",
    "        # Extract sentences\n",
    "        sentences = text_to_sentences(text)\n",
    "\n",
    "        # Get embeddings for the current sentences\n",
    "        sent_embeddings = embeddings[start: start + len(sentences)]\n",
    "\n",
    "        # Predict the best sentence\n",
    "        best_sentence_index = np.argmax(ridge_reg.predict(sent_embeddings))\n",
    "\n",
    "        best_sentences.append(sentences[best_sentence_index])\n",
    "\n",
    "        start += len(sentences)  # Move the pointer to the next span\n",
    "\n",
    "    return best_sentences\n",
    "\n",
    "\n",
    "def avg_score(sentences: list[str], targets: list[str]) -> float:\n",
    "    scores = [\n",
    "        single_rouge_score(target, sentence)\n",
    "        for target, sentence in zip(targets, sentences)\n",
    "    ]\n",
    "    return float(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picked_sentences = pick_sentences(valid_embed, validation_df)\n",
    "avg_score(picked_sentences, validation_df[\"titles\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best regressor gives an average Rouge-L score of 0.1157 on the validation set, which is not very good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression using a neural network\n",
    "\n",
    "Instead, we will use a neural network with fully connected layers in order to predict the Rouge-L score of each sentence relative to their paragraph target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreNN(nn.Module):\n",
    "    \"\"\"Rouge-L predictor\"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.5) # Avoid overfitting\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ScoreNN(input_size=train_embed.shape[1], hidden_size=256).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert numpy arrays to torch tensors\n",
    "train_embed_t = torch.from_numpy(train_embed).float().to(device)\n",
    "train_scores_t = torch.from_numpy(train_scores).float().to(device)\n",
    "\n",
    "valid_embed_t = torch.from_numpy(valid_embed).float().to(device)\n",
    "valid_scores_t = torch.from_numpy(valid_scores).float().to(device)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "batch_size = 10_000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Shuffle the data\n",
    "    indices = torch.randperm(train_embed_t.size(0))\n",
    "\n",
    "    for i in tqdm(range(0, train_embed_t.size(0), batch_size)):\n",
    "        batch_indices = indices[i: i + batch_size]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(train_embed_t[batch_indices]).squeeze()\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, train_scores_t[batch_indices])\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation loss\n",
    "    # Issue: ça prend bcp trop de place à la fois ?\n",
    "    # COmput\n",
    "    # valid_outputs = model(valid_embed_t)\n",
    "    # valid_loss = criterion(valid_outputs, valid_scores_t)\n",
    "    # Compute validation loss by batch\n",
    "    valid_loss = 0\n",
    "    for i in range(0, valid_embed_t.size(0), batch_size):\n",
    "        valid_outputs = model(valid_embed_t[i: i + batch_size]).squeeze()\n",
    "        valid_loss += criterion(\n",
    "            valid_outputs, valid_scores_t[i: i + batch_size]\n",
    "        ).item()\n",
    "\n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item()}, Validation loss: {valid_loss}\")\n",
    "    if epoch % 10 == 0:\n",
    "        # Save the model\n",
    "        torch.save(model.state_dict(), \"data/score_nn.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the same strategy as before to pick the best summarizing sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nrows = validation_df.shape[0]\n",
    "best_sentences: list[str] = []\n",
    "\n",
    "# Embedding span pointers\n",
    "start = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _, (text, *_) in tqdm(validation_df.iterrows(), total=nrows):\n",
    "\n",
    "        # Extract sentences\n",
    "        sentences = text_to_sentences(text)\n",
    "\n",
    "        # Get embeddings for the current sentences\n",
    "        sent_embeddings = valid_embed_t[start: start + len(sentences)]\n",
    "\n",
    "        # Predict the best sentence\n",
    "        best_sentence_index = np.argmax(\n",
    "            model(sent_embeddings).squeeze().cpu().numpy())\n",
    "\n",
    "        best_sentences.append(sentences[best_sentence_index])\n",
    "\n",
    "        start += len(sentences)  # Move the pointer to the next span\n",
    "\n",
    "print(len(best_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_score(best_sentences, validation_df[\"titles\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet again, the score is not that great. It could be improved by using different embeddings and a larger model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
